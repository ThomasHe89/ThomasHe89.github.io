labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainBernoulliNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
N_ct <- sum(sapply(training[labels == classname], function(x) {length(intersect(x, token)) == 1}))
res[["condprob"]][token, as.character(classname)] <- (N_ct+1)/(N_c+2)
}
}
return(res)
}
applyBernoulliNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- unique(d[[i]])
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(V)) {
token <- V[t]
if (token %in% W) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(condprob[token, as.character(classname)])
} else {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(1-condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainBernoulliNB(labels = labels, training = training)
applied <- applyBernoulliNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
# Note that this algorithm factors in non-occurence of terms, i.e.:
# p(c = 1 | d4) = 3/4*1/5*(4/5)^2*(3/5)^3 = 0.020736
# cf.: exp(applied$score[4,2])
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainBernoulliNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
N_ct <- sum(sapply(training[labels == classname], function(x) {length(intersect(x, token)) == 1}))
res[["condprob"]][token, as.character(classname)] <- (N_ct+1)/(N_c+2)
}
}
return(res)
}
applyBernoulliNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- unique(d[[i]])
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(V)) {
token <- V[t]
if (token %in% W) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(condprob[token, as.character(classname)])
} else {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(1-condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainBernoulliNB(labels = labels, training = training)
applied <- applyBernoulliNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
# Note that this algorithm factors in non-occurence of terms, i.e.:
# p(c = 1 | d4) = 3/4*1/5*(4/5)^2*(3/5)^3 = 0.020736
# cf.: exp(applied$score[4,2])
rm(list = ls())
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
trainBernoulliNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
N_ct <- sum(sapply(training[labels == classname], function(x) {length(intersect(x, token)) == 1}))
res[["condprob"]][token, as.character(classname)] <- (N_ct+1)/(N_c+2)
}
}
return(res)
}
applyBernoulliNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- unique(d[[i]])
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(V)) {
token <- V[t]
if (token %in% W) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(condprob[token, as.character(classname)])
} else {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(1-condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
trained <- trainBernoulliNB(labels = labels, training = training)
applied <- applyBernoulliNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
trained
trained$V
rm(list = ls())
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainBernoulliNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
N_ct <- sum(sapply(training[labels == classname], function(x) {length(intersect(x, token)) == 1}))
res[["condprob"]][token, as.character(classname)] <- (N_ct+1)/(N_c+2)
}
}
return(res)
}
applyBernoulliNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- unique(d[[i]])
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(vocab)) {
token <- vocab[t]
if (token %in% W) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(condprob[token, as.character(classname)])
} else {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] +
log(1-condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainBernoulliNB(labels = labels, training = training)
applied <- applyBernoulliNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
# Note that this algorithm factors in non-occurence of terms, i.e.:
# p(c = 1 | d4) = 3/4*1/5*(4/5)^2*(3/5)^3 = 0.020736
# cf.: exp(applied$score[4,2])
applied
exp(applied$score[4,2])
rm(list = ls())
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainMultinomialNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
text_c <- unlist(training[labels == classname])
for (t in 1:length(V)) {
token <- V[t]
T_ct <- sum(text_c == token)
T_ct_not <- length(text_c)
res[["condprob"]][token, as.character(classname)] <- (T_ct+1)/(T_ct_not + length(V))
}
}
return(res)
}
applyMultinomialNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- d[[i]]
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(W)) {
token <- W[t]
if (token %in% vocab) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] + log(condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainMultinomialNB(labels = labels, training = training)
applied <- applyMultinomialNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
rm(list = ls())
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainMultinomialNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
text_c <- unlist(training[labels == classname])
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
T_ct <- sum(text_c == token)
T_ct_not <- length(text_c)
res[["condprob"]][token, as.character(classname)] <- (T_ct+1)/(T_ct_not + length(V))
}
}
return(res)
}
applyMultinomialNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- d[[i]]
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(W)) {
token <- W[t]
if (token %in% vocab) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] + log(condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainMultinomialNB(labels = labels, training = training)
applied <- applyMultinomialNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
rm(list = ls())
### define data
training <- list(c("chinese", "beijing", "chinese"),
c("chinese", "chinese", "shanghai"),
c("chinese", "macao"),
c("tokyo", "japan", "chinese"))
labels <- c(1,1,1,0)
test <- list(c("chinese", "chinese", "chinese", "tokyo", "japan"),
rep("chinese", 5),
rep("tokyo", 5),
c("Deine", "Mutter"))
### define functions
trainMultinomialNB <- function(labels, training) {
res <- vector("list", 3)
names(res) <- c("V", "prior", "condprob")
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res[["V"]] <- unique(unlist(training))
res[["prior"]] <- numeric(length = nclasses)
names(res[["prior"]]) <- c(classnames)
res[["condprob"]] <- data.frame(matrix(nrow = length(res[["V"]]), ncol = nclasses,
dimnames = list(res[["V"]], classnames)),
check.names = FALSE)
N <- length(training)
for (c in 1:length(classnames)) {
classname <- classnames[c]
N_c <- sum(labels == classname)
res[["prior"]][c] <- N_c/N
text_c <- unlist(training[labels == classname])
for (t in 1:length(res[["V"]])) {
token <- res[["V"]][t]
T_ct <- sum(text_c == token)
T_ct_not <- length(text_c)
res[["condprob"]][token, as.character(classname)] <- (T_ct+1)/(T_ct_not + length(res[["V"]]))
}
}
return(res)
}
applyMultinomialNB <- function(labels, vocab, prior, condprob, d) {
classnames <- sort(unique(labels))
nclasses <- length(classnames)
res <- vector("list", 2)
names(res) <- c("score", "pred")
res[["score"]] <- data.frame(matrix(nrow = length(d), ncol = nclasses,
dimnames = list(1:length(d), classnames)),
check.names = FALSE)
for (i in 1:length(d)) {
W <- d[[i]]
for (c in 1:length(classnames)) {
classname <- classnames[c]
res[["score"]][i, as.character(classname)] <- log(prior[as.character(classname)])
for (t in 1:length(W)) {
token <- W[t]
if (token %in% vocab) {
res[["score"]][i, as.character(classname)] <- res[["score"]][i, as.character(classname)] + log(condprob[token, as.character(classname)])
}
}
}
}
res[["pred"]] <- apply(res[["score"]], 1, function(x) names(res[["score"]])[which(x==max(x))])
if (class(classnames) == "numeric") res[["pred"]] <- as.numeric(res[["pred"]])
return(res)
}
### apply functions
trained <- trainMultinomialNB(labels = labels, training = training)
applied <- applyMultinomialNB(labels = labels, vocab = trained$V, prior = trained$prior,
condprob = trained$condprob, d = test)
trained
applied
exp(applied$score)
75.90*.25
res <- read.csv(file = "pitchfork_results.csv", header = FALSE)
setwd("C:/Users/johndoe/OneDrive/2_Master/3. Semester/Python Programming/iPython Notebooks")
require(xlsx)
res <- read.csv(file = "pitchfork_results.csv", header = FALSE)
res
colanmes(res) <- c("rank", "artist", "album", "label", "track", "link")
colnames(res) <- c("rank", "artist", "album", "label", "track", "link")
head(res)
head(res, n = 10L)
head(res, n = 3L)
kable(head(res[,1:3]), format = "markdown")
kable(head(res, n = -10L), format = "markdown")
kable(tail(res, n = 10L), format = "markdown")
tail(res)
kable(tail(res, n = 5L), row.names = FALSE, format = "markdown", longtable = TRUE)
getwd()
setwd("C:/Users/johndoe/Desktop/Dropbox/Sozialwissenschaften/5_Weiteres/github.pages/ThomasHe89.github.io")
servr::jekyll()
servr::jekyll()
servr::jekyll()
library(feather)
install.packages("feather")
getwd()
servr::jekyll()
servr::jekyll()
servr::jekyll()
servr::jekyll()
